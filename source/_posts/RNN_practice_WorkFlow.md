---
title: RNN practice workflow(Pytorch)
tags: ["RNN", "Practice", "Workflow", "Machine Learning"]
---
## RNN
__RNN__ is the abbreviation of _recurrent neural network_. What makes it special is the _memory unit_(h) in its structure, which connects all the operations of one single layer like a chain.

Let's do a comparison between RNN and CNN. In one layer of CNN, though every convolutional operation cares about local correlationship, each of them is independent of one another. In one layer of RNN, every operation will receive the linear operated result of last operation. It makes the output of the layer consider of the whole input instead of part of it. Though it makes RNN have some defects about gradients.

## RNN in pytorch
There are generally two ways of building a RNN layer.
1. `nn.RNN()`
2. `nn.RNNCell()`

Using the first way is convenient while the second way provides us more control of details of the network.

Here are some examples.

1. `nn.RNN()`
```python
import torch
form torch import nn

x = torch.randn(num_words, batch, x_feature)
rnn = nn.RNN(
	input_size=x_feature, 
	hidden_size=h_feature, 
	num_layers=numlayers, 
	batch_first=False)
o, h = rnn(x)
o.shape
#torch.Size([num_words, batch, h_feature])
h.shape
#torch.Size([num_layers, batch, h_feature])
```

There is something strange in the codes above. Because `batch` is not the first dim of the input, and there is a `batch_first=False` by default.
It makes sense when you figure out how RNN works, or you can just insist on the batch-first-style input by setting `batch_first=True` manually.
Another thing that catches our sight is that there are two outputs generated by RNN. `o` is the output of all the operations of the last layer, so it has the same first dim with the input. `h` is the last time of operation's output of every layer, so it has the first dim  the same with the number of layers of the RNN.

2. `nn.RNNCell()`
```python
import torch
from torch import nn

x = torch.randn(num_words, batch, x_feature)
rnn_cell = nn.RNNCell(
	input_size =x_feature,
	hidden_size=h_feature
	)

h_0 = torch.randn(batch, h_feature)
output = []
for i in range(x.shape[0]):
	h_0 = rnn_cell(x[i], h_0)
	output.append(h_0)
```

Understand of the structure above helps the understand of RNN. What the RNNCell do is one single operation of one layer of RNN without reccurent. So we need to update `h_0` mannually to do the job of recurrent. 

The structure and theory of RNN is not difficult to understand but easy to mistake and forget. And there are some brilliant ways to build a flexible RNN since it has high flexibility.

## the flexibility of `nn.RNN`
When using `nn.Linear`, `nn.Conv2d` and many other `nn` layers of pytorch learned before, we can easily notice that these layers don't care about the `batch` dim of the input with a shape of `[batch, channel, h, w]`. It is the advantage of Pytorch which makes the network flexible.

When using `nn.RNN` with a input with a shape of `[num_time_steps, batch, input_feature]`, we can find that `nn.RNN` only cares about the dim of `input_feature`, which means we can do something "magical" if we use our imagination.

## Workflow of the practice

1. data: `sin` in numpy
2. input: 50 continuous value
3. pred: next 1 time step's 50 continuous points' value 

data:
```python
import torch
from torch import nn
import numpy as np
num_time_steps = 50
start = np.random.randint(3, size=1)
time_steps = np.linspace(start, start +10, num_time_steps)
data = np.sin(time_steps)
data = data.reshape(num_time_steps, 1)
x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)
y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)
# in this case, we use batch_first=True in the RNN 
# data[:-1] will select all the elements of the array except the last one
# data[1:]  all the elements except the first one(0)
```
Network:

```python
class Net(nn.Module):
	def __init__(self, ):
		super(Net, self).__init__()
		self.rnn = nn.RNN(
			input_size=1,
			hidden_size=10,
			num_layers=1,
			batch_first=True,
			)
		self.linear = nn.Linear(10, 1)
	def forward(self, x, hidden_prev):
		out, hidden_prev = self.rnn(x, hidden_prev)
		# flatten
		out = out.view(-1, 10)
		out = nn.linear(out)
		# unsqueeze batch's dim
		out = out.unsqueeze(dim=0)
		return out, hidden_prev
		
```

train:
```python
import torch
from torch import nn
import numpy as np
class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.rnn = nn.RNN(
			input_size=1,
			hidden_size=10,
			num_layers=2,
			batch_first=True
		)
		self.linear = nn.Linear(10, 1)
	def forward(self, x, hidden_prev):
		# x: [1, time_steps, input_feature]
		out, hidden_prev = self.rnn(x, hidden_prev)
		# out: [1, time_steps, hidden_feature]
		# hidden_prev: [1, num_layers, hidden_feature]
		out = out.view(-1, hidden_feature)
		# out: [time_steps * batch, hidden_feature]
		out = self.linear(out)
		# out: [time_steps * batch, 1]
		out = out.unsqueeze(dim=0)
		return out, hidden_prev

model = Net()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
hidden_prev = torch.zeros(1, 2, 10)
#h0: [batch, num_layers, hidden_feature]
num_time_steps = 50
for iter in range(6000):
	start = np.random.randint(3, size=1)
	time_steps = np.linspace(start, start + 10, num_time_steps)
	data = np.sin(time_steps)
	x = torch.tensor(data[:-1]).float().view(1, num_time_steps -1 ,1)
	y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)

	output, hidden_prev = model(x, hidden_prev)
	hidden_prev = hidden_prev.detach()
	# .detach() set the requires_grad=False of hidden_prev forever
	# Here I can not understand of the action of not clear the hidden_prev of the current loop
	
	loss = criterion(output, y)
	optimizer.zero_grad()
	loss.backward()
	optimizer.step()

	if iter%100 == 0:
		print("iter:{}, loss:{}".format(iter, loss.item()))

prediction = []
start = np.random.randint(3, size=1)
time_steps = np.linspace(start, start + 10, num_time_steps)
data = np.sin(time_steps)
x = torch.tensor(data[]).float().view(1, num_time_steps - 1, 1)
input = x[:, 0, :]
for _ in range(x.shape[1]):
	input = input.view(1, 1, 1)
	pred, hidden_prev = model(input, hidden_prev)
	input = pred
	# update the input for predicting the next y
	predictions.append(pred.detach().numpy.ravel()[0])
```

We have mentioned the flexibility of RNN about input before. In the prediction part of the codes above, we can notice that we can take control of the shape of the output by proper manipulation.
The prediction codes above, we reshape the output by manual updation of `hidden_prev` and `input`, `append` and loop. There are some details that I am not very clear about, like `.detach()`.
