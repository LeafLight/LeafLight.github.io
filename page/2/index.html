<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>LeafLight&#39;s Blog by Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="LeafLight&#39;s Blog by Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="LeafLight&#39;s Blog by Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="LeafLight">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="LeafLight&#39;s Blog by Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">LeafLight&#39;s Blog by Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-RNN_practice_WorkFlow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/08/09/RNN_practice_WorkFlow/" class="article-date">
  <time datetime="2022-08-08T17:51:44.728Z" itemprop="datePublished">2022-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/08/09/RNN_practice_WorkFlow/">RNN practice workflow(Pytorch)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p><strong>RNN</strong> is the abbreviation of <em>recurrent neural network</em>. What makes it special is the <em>memory unit</em>(h) in its structure, which connects all the operations of one single layer like a chain.</p>
<p>Let’s do a comparison between RNN and CNN. In one layer of CNN, though every convolutional operation cares about local correlationship, each of them is independent of one another. In one layer of RNN, every operation will receive the linear operated result of last operation. It makes the output of the layer consider of the whole input instead of part of it. Though it makes RNN have some defects about gradients.</p>
<h2 id="RNN-in-pytorch"><a href="#RNN-in-pytorch" class="headerlink" title="RNN in pytorch"></a>RNN in pytorch</h2><p>There are generally two ways of building a RNN layer.</p>
<ol>
<li><code>nn.RNN()</code></li>
<li><code>nn.RNNCell()</code></li>
</ol>
<p>Using the first way is convenient while the second way provides us more control of details of the network.</p>
<p>Here are some examples.</p>
<ol>
<li><code>nn.RNN()</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">form torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.randn(num_words, batch, x_feature)</span><br><span class="line">rnn = nn.RNN(</span><br><span class="line">	input_size=x_feature, </span><br><span class="line">	hidden_size=h_feature, </span><br><span class="line">	num_layers=numlayers, </span><br><span class="line">	batch_first=<span class="literal">False</span>)</span><br><span class="line">o, h = rnn(x)</span><br><span class="line">o.shape</span><br><span class="line"><span class="comment">#torch.Size([num_words, batch, h_feature])</span></span><br><span class="line">h.shape</span><br><span class="line"><span class="comment">#torch.Size([num_layers, batch, h_feature])</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>There is something strange in the codes above. Because <code>batch</code> is not the first dim of the input, and there is a <code>batch_first=False</code> by default.<br>It makes sense when you figure out how RNN works, or you can just insist on the batch-first-style input by setting <code>batch_first=True</code> manually.<br>Another thing that catches our sight is that there are two outputs generated by RNN. <code>o</code> is the output of all the operations of the last layer, so it has the same first dim with the input. <code>h</code> is the last time of operation’s output of every layer, so it has the first dim  the same with the number of layers of the RNN.</p>
<ol start="2">
<li><code>nn.RNNCell()</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.randn(num_words, batch, x_feature)</span><br><span class="line">rnn_cell = nn.RNNCell(</span><br><span class="line">	input_size =x_feature,</span><br><span class="line">	hidden_size=h_feature</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">h_0 = torch.randn(batch, h_feature)</span><br><span class="line">output = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">	h_0 = rnn_cell(x[i], h_0)</span><br><span class="line">	output.append(h_0)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Understand of the structure above helps the understand of RNN. What the RNNCell do is one single operation of one layer of RNN without reccurent. So we need to update <code>h_0</code> mannually to do the job of recurrent. </p>
<p>The structure and theory of RNN is not difficult to understand but easy to mistake and forget. And there are some brilliant ways to build a flexible RNN since it has high flexibility.</p>
<h2 id="the-flexibility-of-nn-RNN"><a href="#the-flexibility-of-nn-RNN" class="headerlink" title="the flexibility of nn.RNN"></a>the flexibility of <code>nn.RNN</code></h2><p>When using <code>nn.Linear</code>, <code>nn.Conv2d</code> and many other <code>nn</code> layers of pytorch learned before, we can easily notice that these layers don’t care about the <code>batch</code> dim of the input with a shape of <code>[batch, channel, h, w]</code>. It is the advantage of Pytorch which makes the network flexible.</p>
<p>When using <code>nn.RNN</code> with a input with a shape of <code>[num_time_steps, batch, input_feature]</code>, we can find that <code>nn.RNN</code> only cares about the dim of <code>input_feature</code>, which means we can do something “magical” if we use our imagination.</p>
<h2 id="Workflow-of-the-practice"><a href="#Workflow-of-the-practice" class="headerlink" title="Workflow of the practice"></a>Workflow of the practice</h2><ol>
<li>data: <code>sin</code> in numpy</li>
<li>input: 50 continuous value</li>
<li>pred: next 1 time step’s 50 continuous points’ value </li>
</ol>
<p>data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">num_time_steps = <span class="number">50</span></span><br><span class="line">start = np.random.randint(<span class="number">3</span>, size=<span class="number">1</span>)</span><br><span class="line">time_steps = np.linspace(start, start +<span class="number">10</span>, num_time_steps)</span><br><span class="line">data = np.sin(time_steps)</span><br><span class="line">data = data.reshape(num_time_steps, <span class="number">1</span>)</span><br><span class="line">x = torch.tensor(data[:-<span class="number">1</span>]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.tensor(data[<span class="number">1</span>:]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># in this case, we use batch_first=True in the RNN </span></span><br><span class="line"><span class="comment"># data[:-1] will select all the elements of the array except the last one</span></span><br><span class="line"><span class="comment"># data[1:]  all the elements except the first one(0)</span></span><br></pre></td></tr></table></figure>
<p>Network:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, </span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.rnn = nn.RNN(</span><br><span class="line">			input_size=<span class="number">1</span>,</span><br><span class="line">			hidden_size=<span class="number">10</span>,</span><br><span class="line">			num_layers=<span class="number">1</span>,</span><br><span class="line">			batch_first=<span class="literal">True</span>,</span><br><span class="line">			)</span><br><span class="line">		self.linear = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, hidden_prev</span>):</span></span><br><span class="line">		out, hidden_prev = self.rnn(x, hidden_prev)</span><br><span class="line">		<span class="comment"># flatten</span></span><br><span class="line">		out = out.view(-<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">		out = nn.linear(out)</span><br><span class="line">		<span class="comment"># unsqueeze batch&#x27;s dim</span></span><br><span class="line">		out = out.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">		<span class="keyword">return</span> out, hidden_prev</span><br><span class="line">		</span><br></pre></td></tr></table></figure>

<p>train:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.rnn = nn.RNN(</span><br><span class="line">			input_size=<span class="number">1</span>,</span><br><span class="line">			hidden_size=<span class="number">10</span>,</span><br><span class="line">			num_layers=<span class="number">2</span>,</span><br><span class="line">			batch_first=<span class="literal">True</span></span><br><span class="line">		)</span><br><span class="line">		self.linear = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, hidden_prev</span>):</span></span><br><span class="line">		<span class="comment"># x: [1, time_steps, input_feature]</span></span><br><span class="line">		out, hidden_prev = self.rnn(x, hidden_prev)</span><br><span class="line">		<span class="comment"># out: [1, time_steps, hidden_feature]</span></span><br><span class="line">		<span class="comment"># hidden_prev: [1, num_layers, hidden_feature]</span></span><br><span class="line">		out = out.view(-<span class="number">1</span>, hidden_feature)</span><br><span class="line">		<span class="comment"># out: [time_steps * batch, hidden_feature]</span></span><br><span class="line">		out = self.linear(out)</span><br><span class="line">		<span class="comment"># out: [time_steps * batch, 1]</span></span><br><span class="line">		out = out.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">		<span class="keyword">return</span> out, hidden_prev</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">hidden_prev = torch.zeros(<span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment">#h0: [batch, num_layers, hidden_feature]</span></span><br><span class="line">num_time_steps = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6000</span>):</span><br><span class="line">	start = np.random.randint(<span class="number">3</span>, size=<span class="number">1</span>)</span><br><span class="line">	time_steps = np.linspace(start, start + <span class="number">10</span>, num_time_steps)</span><br><span class="line">	data = np.sin(time_steps)</span><br><span class="line">	x = torch.tensor(data[:-<span class="number">1</span>]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps -<span class="number">1</span> ,<span class="number">1</span>)</span><br><span class="line">	y = torch.tensor(data[<span class="number">1</span>:]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	output, hidden_prev = model(x, hidden_prev)</span><br><span class="line">	hidden_prev = hidden_prev.detach()</span><br><span class="line">	<span class="comment"># .detach() set the requires_grad=False of hidden_prev forever</span></span><br><span class="line">	<span class="comment"># Here I can not understand of the action of not clear the hidden_prev of the current loop</span></span><br><span class="line">	</span><br><span class="line">	loss = criterion(output, y)</span><br><span class="line">	optimizer.zero_grad()</span><br><span class="line">	loss.backward()</span><br><span class="line">	optimizer.step()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">iter</span>%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;iter:&#123;&#125;, loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">iter</span>, loss.item()))</span><br><span class="line"></span><br><span class="line">prediction = []</span><br><span class="line">start = np.random.randint(<span class="number">3</span>, size=<span class="number">1</span>)</span><br><span class="line">time_steps = np.linspace(start, start + <span class="number">10</span>, num_time_steps)</span><br><span class="line">data = np.sin(time_steps)</span><br><span class="line">x = torch.tensor(data[]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = x[:, <span class="number">0</span>, :]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">	<span class="built_in">input</span> = <span class="built_in">input</span>.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">	pred, hidden_prev = model(<span class="built_in">input</span>, hidden_prev)</span><br><span class="line">	<span class="built_in">input</span> = pred</span><br><span class="line">	<span class="comment"># update the input for predicting the next y</span></span><br><span class="line">	predictions.append(pred.detach().numpy.ravel()[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>We have mentioned the flexibility of RNN about input before. In the prediction part of the codes above, we can notice that we can take control of the shape of the output by proper manipulation.<br>The prediction codes above, we reshape the output by manual updation of <code>hidden_prev</code> and <code>input</code>, <code>append</code> and loop. There are some details that I am not very clear about, like <code>.detach()</code>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/08/09/RNN_practice_WorkFlow/" data-id="clsk6sw3e000ncf26aastej7g" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Practice/" rel="tag">Practice</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Workflow/" rel="tag">Workflow</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Transformer_learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/08/09/Transformer_learning/" class="article-date">
  <time datetime="2022-08-08T17:51:44.728Z" itemprop="datePublished">2022-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/08/09/Transformer_learning/">A deeper insight into the training of Transformer Autoencoder</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Transformer-Abstract"><a href="#Transformer-Abstract" class="headerlink" title="Transformer Abstract"></a>Transformer Abstract</h2><p><em>Transformer</em> is a well-known neural network architecture, I have learned about this before from Mu Li’s <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE">video on bilibili</a>. The video is brief but cover the details about the attention mechanism the <em>Transformer</em> uses. </p>
<p>However, When I want to practice using Pytorch code following another <a target="_blank" rel="noopener" href="https://m.bilibili.com/video/BV19Y411b7qx">video</a>, the training way of Transformer is a little different with normal CNN. It is like RNN or LSTM, but I am not very familiar with NLP and associated architectures.</p>
<h2 id="Transformer-training"><a href="#Transformer-training" class="headerlink" title="Transformer training"></a>Transformer training</h2><p>In the practice video above, what confused me is <em>p9</em>(the 9th video). </p>
<p>Q1: The video’s task is translation between language <em>x</em> and <em>y</em>. But the shape of <em>x sentence</em> is <code>[b, 50, e]</code> while that of <em>y sentence</em> is <code>[b, 51, e]</code>. </p>
<p>Q2: Then when training the model <code>y[:, :-1, :]</code> was used as target, while <code>y[:, 1:, :]</code> was used to do loss calculation.</p>
<p>Q3: When using the model to do <code>pred(x)</code>(which returns y, referred as <code>out</code> to avoid mistaking), the <code>out</code> was generated one word by one word.</p>
<h2 id="Understanding"><a href="#Understanding" class="headerlink" title="Understanding"></a>Understanding</h2><p>A1: It’s for the convenience of slicing. The reason for slicing is to do things about Q2.</p>
<p>A2: You may notice that by the special way of slicing, the target and loss label were one-position crossing. With the help of <code>tril_mask</code>, the model is able to learn to predict the next word given the preceding word.</p>
<p>A3: Since its ability is to predict the next one word, the model has to do prediction in this way.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/08/09/Transformer_learning/" data-id="clsk6sw3e000rcf268kzbch1o" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/08/09/hello-world/" class="article-date">
  <time datetime="2022-08-08T17:51:44.728Z" itemprop="datePublished">2022-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/08/09/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/08/09/hello-world/" data-id="clsk6sw3g0014cf2646720m4q" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ClassificationAndMNIST" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/08/09/ClassificationAndMNIST/" class="article-date">
  <time datetime="2022-08-08T17:51:44.727Z" itemprop="datePublished">2022-08-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MachineLearning/">MachineLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/08/09/ClassificationAndMNIST/">Classification and MNIST dataset</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="About-the-input"><a href="#About-the-input" class="headerlink" title="About the input:"></a>About the input:</h2><p>It is a famous dataset of Human-writed number pictures used for deep learning.</p>
<ul>
<li>size of one picture: 28x28(pixels)</li>
<li>quantity: 7000 per Number(0~9) (7kx10)</li>
</ul>
<h2 id="About-the-output"><a href="#About-the-output" class="headerlink" title="About the output:"></a>About the output:</h2><p>If we simply use 0<del>9 to represent the result of prediction of our model, it results in a problem that 0</del>9 are quantitive variables. To avoid it, Here is a new method called <strong>One-Hot</strong>, which is useful in classification.</p>
<h3 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h3><h4 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h4><p>It is an special encoding method, which use a matrix(usually has an dimension of 1xn) to represent a categorical variable.</p>
<p>Here comes an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">red = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">blue = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">yellow = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#So we can use [1,0,0,0,0,0,0,0,0,0] to represent &#x27;0&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="Evaluation-loss-calculation"><a href="#Evaluation-loss-calculation" class="headerlink" title="Evaluation(loss calculation)"></a>Evaluation(<em>loss</em> calculation)</h4><p>As to calculate the <em>loss</em> of the model, <em>Euclidean distance</em> is used(because the result of One-Hot Matrix in this model has a dimension of 10x1, which means <em>Euclidean distance</em> is proper).</p>
<p><em>here remains a question– why Euclidean distance is not proper for all kinds of data</em></p>
<p>Here comes an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Real_label = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">Pred = [<span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line">loss =<span class="built_in">sum</span>([<span class="built_in">pow</span>(Real_label[i]-Pred[i],<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">4</span>)]) </span><br></pre></td></tr></table></figure>

<h2 id="About-the-model"><a href="#About-the-model" class="headerlink" title="About the model:"></a>About the model:</h2><h3 id="Sigmoid-function-and-ReLU-function"><a href="#Sigmoid-function-and-ReLU-function" class="headerlink" title="Sigmoid function and ReLU function:"></a>Sigmoid function and ReLU function:</h3><ul>
<li>Sigmoid function:<br><img src="https://www.google.com/imgres?imgurl=https://miro.medium.com/max/1200/1*a04iKNbchayCAJ7-0QlesA.png&imgrefurl=https://medium.com/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e&tbnid=Zl9O5xKIlo6tvM&vet=12ahUKEwjH99qG-tb1AhXYqnIEHciXB-oQMygBegUIARDaAQ..i&docid=HgiWI3njmHtosM&w=1200&h=630&itg=1&q=Sigmoid%20function&hl=en-US&client=ubuntu&ved=2ahUKEwjH99qG-tb1AhXYqnIEHciXB-oQMygBegUIARDaAQ" alt="Sigmoid function"></li>
</ul>
<p>$$ h_ \theta (x) =  \frac{\mathrm{1} }{\mathrm{1} + e^- \theta^Tx }  $$</p>
<ul>
<li>ReLU function:<br><img src="https://www.google.com/imgres?imgurl=https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png&imgrefurl=https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec&tbnid=0UdUiZ4X2VLDiM&vet=12ahUKEwifmaTS-tb1AhWHq3IEHS10CgoQMygBegUIARC-AQ..i&docid=8NiVbpcoDLL_LM&w=357&h=278&itg=1&q=ReLU%20function&hl=en-US&client=ubuntu&ved=2ahUKEwifmaTS-tb1AhWHq3IEHS10CgoQMygBegUIARC-AQ" alt="ReLU function"></li>
</ul>
<p>$$ R(x) = max(0,x) $$</p>
<p><em>Why they are special: We don’t use linear function because linear factor is not vey qualified to deal with complex REAL-WORLD problem(Though recognizing a hand-writing number is easy for us human beings, it is difficult to teach the machine this technique.). Sigmoid F and ReLU F have a better simulation of the nerves of human beings, which have thresholds.</em> </p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><ul>
<li><p>$$ Objective = \sum (pred - Y) $$</p>
</li>
<li><p>minimize objective </p>
</li>
<li><p>for a new $$X$$</p>
<ul>
<li><p>$$[W_1,W_2,W_3]$$</p>
</li>
<li><p>$$[b_1,b_2,b_3]$$</p>
</li>
<li><p>$$pred = W_3 * \left{ W_2 \left[ W_1X + b_1\right] +b_2\right} + b_3$$</p>
</li>
<li><p>$$argmax(pred)$$</p>
</li>
</ul>
</li>
</ul>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>MNIST_utils.py </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_curve</span>(<span class="params">data</span>):</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(data)), data, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;value&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">img, label, name, num=(<span class="params"><span class="number">4</span>,<span class="number">4</span></span>), dataset_MU=<span class="number">0.1307</span>, dataset_SIG=<span class="number">0.3081</span></span>):</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num[<span class="number">0</span>]*num[<span class="number">1</span>]):</span><br><span class="line">        plt.subplot(num[<span class="number">0</span>], num[<span class="number">1</span>], i+<span class="number">1</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        plt.imshow(img[i][<span class="number">0</span>]*dataset_SIG+dataset_MU, cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(name, label[i].item()))</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">label, depth=<span class="number">10</span></span>):</span></span><br><span class="line">    out = torch.zeros(label.size(<span class="number">0</span>), depth)</span><br><span class="line">    idx = torch.LongTensor(label).view(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    out.scatter_(dim=<span class="number">1</span>, index=idx, value=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>MNIST_train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> MNIST_utils <span class="keyword">import</span> plot_image, plot_curve, one_hot</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">dataset_MU = <span class="number">0.1307</span></span><br><span class="line">dataset_SIG = <span class="number">0.3081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step1. load dataset</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        torchvision.datasets.MNIST(<span class="string">&#x27;mnist.data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                    transform=torchvision.transforms.Compose([</span><br><span class="line">                                        torchvision.transforms.ToTensor(),</span><br><span class="line">                                        torchvision.transforms.Normalize(</span><br><span class="line">                                            (dataset_MU,),(dataset_SIG,))</span><br><span class="line">                                        ])</span><br><span class="line">        ),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">        torchvision.datasets.MNIST(<span class="string">&#x27;mnist.data/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                    transform=torchvision.transforms.Compose([</span><br><span class="line">                                        torchvision.transforms.ToTensor(),</span><br><span class="line">                                        torchvision.transforms.Normalize(</span><br><span class="line">                                            (dataset_MU,),(dataset_SIG,))</span><br><span class="line">                                        </span><br><span class="line">                                        ])</span><br><span class="line">        ),</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">sample_x, sample_y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="built_in">print</span>(sample_x.shape, sample_y.shape)</span><br><span class="line"><span class="comment">#Output[0]:torch.Size([512, 1, 28, 28]) torch.Size([512])</span></span><br><span class="line">plot_image(sample_x,sample_y,<span class="string">&#x27;image sample&#x27;</span>,num=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#plot_image() is the function imported from MNIST_utils.py</span></span><br><span class="line"><span class="comment">#Output[1]:some hand-writing numbers&#x27; pictures(default 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step2. Network model building</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="comment"># 28*28: the pixel size of one picture in the dataset</span></span><br><span class="line">        <span class="comment"># 256: hyper para.</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="comment"># 256: the size of output of the prior layer</span></span><br><span class="line">        <span class="comment"># 64: hyper para.</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        <span class="comment"># 64: the size of output of the prior layer</span></span><br><span class="line">        <span class="comment"># 10: the size of final output</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment">#x: [b, 1, 28, 28]</span></span><br><span class="line">        <span class="comment">#h1 = relu(w1x+b1)</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment">#h2 = relu(w2h1+b2)</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment">#h3 = (w3h2+b3)</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#initialize the network</span></span><br><span class="line">net = Net()</span><br><span class="line"><span class="comment">#[w1, w2, w3, b1, b2, b3]</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span> )</span><br><span class="line"><span class="comment">#para. used to plot the loss_step plot</span></span><br><span class="line">train_loss = []</span><br><span class="line"><span class="comment"># step3. Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="comment">#3 times of traversal of the train set</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment">#print(x.shape, y.shape)</span></span><br><span class="line">        <span class="comment">#Output[2]: x:[b, 1, 28, 28], y:[512]</span></span><br><span class="line">        <span class="comment"># [b, 1, 28, 28] =&gt; [b, feature]</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="comment"># =&gt;[b, 10]</span></span><br><span class="line">        out = net(x)</span><br><span class="line">        <span class="comment"># [b, 10]</span></span><br><span class="line">        y_onehot = one_hot(y)</span><br><span class="line">        <span class="comment"># loss = mse(out, y_onehot)</span></span><br><span class="line">        loss = F.mse_loss(out, y_onehot)</span><br><span class="line">        <span class="comment"># clean the gradient</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># calculate the gradient</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update the gradient</span></span><br><span class="line">        <span class="comment">#w&#x27; = w- lr*grad</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment">#para. &#x27;loss&#x27; is a tensor object, use .item() to convert it to numpy object</span></span><br><span class="line">        train_loss.append(loss.item())</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(epoch, batch_idx, loss.item())</span><br><span class="line"><span class="comment"># we get the optimal [w1, b1, w2, b2, w3, b3]</span></span><br><span class="line">plot_curve(train_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step4. test</span></span><br><span class="line">total_correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> test_loader:</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    out = net(x)</span><br><span class="line">    <span class="comment">#out: [b, 10] =&gt; pred: [b]</span></span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    correct = pred.eq(y).<span class="built_in">sum</span>().<span class="built_in">float</span>()</span><br><span class="line">    total_correct += correct</span><br><span class="line"></span><br><span class="line">total_num = <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">acc = total_correct / total_num</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test acc:&#x27;</span>, acc)</span><br><span class="line"></span><br><span class="line">x, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line">out = net(x.view(x.size(<span class="number">0</span>),<span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">plot_image(x, pred, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/08/09/ClassificationAndMNIST/" data-id="clsk6sw3a0004cf269l747jk7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Note/" rel="tag">Note</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Source-Code/" rel="tag">Source Code</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-nltk-Grammar" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/16/nltk-Grammar/" class="article-date">
  <time datetime="2022-04-16T13:14:59.000Z" itemprop="datePublished">2022-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MachineLearning/">MachineLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/16/nltk-Grammar/">nltk and Grammar -- Encoding Part</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>When dealing with the recent project associated with <em>CMap</em>, an interesting neural network model called <strong>GVAE</strong> caught my attention. After learning the details of it, I tried to re-do the model and this blog is a recording in some ways and mainly about the <em>Grammar</em> part.</p>
<h2 id="Grammar-–-Context-Free-Grammar-CFG"><a href="#Grammar-–-Context-Free-Grammar-CFG" class="headerlink" title="Grammar – Context-Free Grammar(CFG)"></a>Grammar – Context-Free Grammar(CFG)</h2><p>The key feature of GVAE is <strong>CFG</strong>, which can be manipulated easily by the python module <code>nltk</code>.</p>
<ol>
<li><p>Generate a <code>CFG</code> object from string</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> CFG</span><br><span class="line"></span><br><span class="line">SMILEsGrammar = CFG.fromstring(</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    smiles -&gt; chain</span></span><br><span class="line"><span class="string">    atom -&gt; bracket_atom | aliphatic_organic | aromatic_organic</span></span><br><span class="line"><span class="string">    aliphatic_organic -&gt; &#x27;B&#x27; | &#x27;C&#x27; | &#x27;N&#x27; | &#x27;O&#x27; | &#x27;S&#x27; | &#x27;P&#x27; | &#x27;F&#x27; | &#x27;I&#x27; | &#x27;Cl&#x27; | &#x27;Br&#x27;</span></span><br><span class="line"><span class="string">    aromatic_organic -&gt; &#x27;[&#x27; BAI &#x27;]&#x27;</span></span><br><span class="line"><span class="string">    BAI -&gt; isotope symbol BAC | symbol BAC | isotope symbol | symbol</span></span><br><span class="line"><span class="string">    BAC -&gt; chiral BAH | BAH | chiral</span></span><br><span class="line"><span class="string">    BAH -&gt; hcount BACH | BACH | hcount</span></span><br><span class="line"><span class="string">    BACH -&gt; charge class | charge | class</span></span><br><span class="line"><span class="string">    symbol -&gt; aliphatic_organic | aromatic_organic</span></span><br><span class="line"><span class="string">    isotope -&gt; DIGIT | DIGIT DIGIT | DIGIT DIGIT DIGIT</span></span><br><span class="line"><span class="string">    DIGIT -&gt; &#x27;1&#x27; | &#x27;2&#x27; | &#x27;3&#x27; | &#x27;4&#x27; | &#x27;5&#x27; | &#x27;6&#x27; | &#x27;7&#x27; | &#x27;8&#x27;</span></span><br><span class="line"><span class="string">    chiral -&gt; &#x27;@&#x27; | &#x27;@@&#x27;</span></span><br><span class="line"><span class="string">    hcount -&gt; &#x27;H&#x27; | &#x27;H&#x27; DIGIT</span></span><br><span class="line"><span class="string">    charge -&gt; &#x27;-&#x27; | &#x27;-&#x27; DIGIT | &#x27;-&#x27; DIGIT DIGIT | &#x27;+&#x27; | &#x27;+&#x27; DIGIT | &#x27;+&#x27; DIGIT DIGIT</span></span><br><span class="line"><span class="string">    bond -&gt; &#x27;-&#x27; | &#x27;=&#x27; | &#x27;#&#x27; | &#x27;/&#x27; | &#x27;\\&#x27;</span></span><br><span class="line"><span class="string">    ringbond -&gt; DIGIT | bond DIGIT</span></span><br><span class="line"><span class="string">    branched_atom -&gt; atom | atom RB | atom BB | atom RB BB</span></span><br><span class="line"><span class="string">    RB -&gt; RB ringbond | ringbond</span></span><br><span class="line"><span class="string">    BB -&gt; BB branch | branch</span></span><br><span class="line"><span class="string">    branch -&gt; &#x27;(&#x27; chain &#x27;)&#x27; | &#x27;(&#x27; bond chain &#x27;)&#x27;</span></span><br><span class="line"><span class="string">    chain -&gt; branched_atom | chain branched_atom | chain bond branched_atom</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li><p>Generate a parse tree of a molecule in the form of SMILEs</p>
<ol>
<li>Get a tokenizer<br> Since some leaves in the grammar has more than one charactor(like “Cl” or “Br”), default tokenizer may result in errors. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_zinc_tokenizer</span>(<span class="params">cfg</span>):</span></span><br><span class="line">    <span class="comment"># get all the long tokens for the following replacement work</span></span><br><span class="line">    <span class="comment"># long tokens: tokens with more than one charactor, like &#x27;Br&#x27;</span></span><br><span class="line">    long_tokens = [a <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">list</span>(SMILEsGrammar._lexical_index.keys()) <span class="keyword">if</span> xlength(a) &gt; <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># char used for replacements of &#x27;Cl&#x27;, &#x27;Br&#x27;, &#x27;@@&#x27;</span></span><br><span class="line">    replacements = [<span class="string">&#x27;$&#x27;</span>, <span class="string">&#x27;%&#x27;</span>, <span class="string">&#x27;^&#x27;</span>]</span><br><span class="line">    <span class="comment"># ensure that we have  paired origin tokens and their replacements</span></span><br><span class="line">    <span class="keyword">assert</span> xlength(long_tokens) == <span class="built_in">len</span>(replacements)</span><br><span class="line">    <span class="comment"># ensure that all the tokens for replacement is available: not in the origin dict of grammar</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> replacements:</span><br><span class="line">        <span class="keyword">assert</span> token <span class="keyword">not</span> <span class="keyword">in</span> cfg._lexical_index</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the func to return</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">smiles</span>):</span></span><br><span class="line">        <span class="comment"># replace all the long_tokens in the input SMILEs</span></span><br><span class="line">        <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(long_tokens):</span><br><span class="line">            smiles = smiles.replace(token, replacements[i])</span><br><span class="line">        <span class="comment"># the result variable init</span></span><br><span class="line">        tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> smiles:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># try to find the replaced elements&#x27; index and append the original elements </span></span><br><span class="line">                ix = replacements.index(token)</span><br><span class="line">                tokens.append(long_tokens[ix])</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                tokens.append(token)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line">    <span class="keyword">return</span> tokenize</span><br></pre></td></tr></table></figure></li>
<li>tokenize the SMILEs string<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">smi = <span class="string">&quot;CC1=CC(=O)C2=C(O1)C=C3C(=C2OC)C=CO3&quot;</span></span><br><span class="line">SMILEs_tokenizer = get_zinc_tokenizer(SMILEsGrammar)</span><br><span class="line">smi_t = SMILEs_tokenizer(smi)</span><br></pre></td></tr></table></figure></li>
<li>generate the parse tree of the given SMILEs string<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SMILEs_parser = nltk.ChartParser(SMILEsGrammar)</span><br><span class="line">smi_s = <span class="built_in">next</span>(SMILEs_parser.parse(smi_t))</span><br><span class="line"><span class="built_in">type</span>(smi_s)</span><br><span class="line"><span class="comment"># nltk.tree.tree.Tree</span></span><br></pre></td></tr></table></figure></li>
<li>regenerate the SMILEs from the tree<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;</span>.join(smi_s.leaves()</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>Get the productions-index map dict for one-hot encode</p>
<ol>
<li>productions of the parse tree<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smi_s.productions()</span><br></pre></td></tr></table></figure></li>
<li>productions-index map dict<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Prod_map = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> ix, prod <span class="keyword">in</span> <span class="built_in">enumerate</span>(SMILEsGrammar.productions()):</span><br><span class="line">	Prod_map[prod] = ix</span><br></pre></td></tr></table></figure></li>
<li>one-hot encoding<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># a batch of smiles strings as example</span></span><br><span class="line">smiles = <span class="string">&quot;here is a list of smiles&quot;</span></span><br><span class="line"></span><br><span class="line">smiles_t = <span class="built_in">map</span>(SMILEs_tokenizer, smiles)</span><br><span class="line"></span><br><span class="line">smiles_parse_trees = []</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(smiles_t):</span><br><span class="line">	smiles_parse_trees[i] = SMILEs_parser.parse(t)</span><br><span class="line"></span><br><span class="line">productions_seq = [tree.productions() <span class="keyword">for</span> tree <span class="keyword">in</span> smiles_parse_trees]</span><br><span class="line"></span><br><span class="line">indices = [np.array([Prod_map[prod] <span class="keyword">for</span> prod <span class="keyword">in</span> entry], dtype=<span class="built_in">int</span>) <span class="keyword">for</span> entry <span class="keyword">in</span> productions_seq]</span><br><span class="line"></span><br><span class="line">MAX_LEN = <span class="number">277</span></span><br><span class="line">n_char = <span class="built_in">len</span>(SMILEsGrammar.productions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># init </span></span><br><span class="line">one_hot = np.zeros((<span class="built_in">len</span>(indices), MAX_LEN, n_char),dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(indices)):</span><br><span class="line">	num_productions = <span class="built_in">len</span>(indices[i])</span><br><span class="line">	<span class="keyword">if</span> num_productions &gt; MAX_LEN:</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;Too Large molecule, out of range&quot;</span>)</span><br><span class="line">		one_hot[i][np.arange(MAX_LEN), indices[i][:MAX_LEN]] = <span class="number">1</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		one_hot[i][np.arange(num_productions), indices[i]] = <span class="number">1</span></span><br><span class="line">		one_hot[i][np.arange(num_productions, MAX_LEN, -<span class="number">1</span>)] = <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/16/nltk-Grammar/" data-id="clsk6sw3f000ucf26b2kjayit" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CMap/" rel="tag">CMap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GrammarVAE/" rel="tag">GrammarVAE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/molecules/" rel="tag">molecules</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nltk/" rel="tag">nltk</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-xlength-interesting-function" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/10/xlength-interesting-function/" class="article-date">
  <time datetime="2022-04-10T06:26:26.000Z" itemprop="datePublished">2022-04-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/10/xlength-interesting-function/">xlength-an interesting function</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>When reading the source code of <a target="_blank" rel="noopener" href="https://github.com/kekegg/DLEPS/bolb/main/code/Preprocess/preprocess.ipynb">DLEPS</a>, some interesting codes caught my eyes, including: <code>six.string_types</code>, <code>get_zinc_tokenizer()</code>, <code>xlength()</code>, and so on.</p>
<h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><h3 id="Pre-knowledge"><a href="#Pre-knowledge" class="headerlink" title="Pre-knowledge"></a>Pre-knowledge</h3><h4 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h4><p>A lambda function in <em>python</em> is in some ways like a simple <code>def</code> fucntion with only a <code>return</code> line.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use `def` function to represent `lambda x: x * x`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x * x</span><br></pre></td></tr></table></figure>

<h4 id="functools-reduce"><a href="#functools-reduce" class="headerlink" title="functools.reduce()"></a>functools.reduce()</h4><p><code>functools</code> is a python module which contains some advanced functions. The <code>functools.reduce()</code> will apply a given function to a iterable sequence and a initial value is optional.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use `functools.reduce()` to realize `sum()`</span></span><br><span class="line"><span class="comment"># functools.reduce(func, iter[, init])</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="built_in">sum</span> = reduce(<span class="keyword">lambda</span> x1, x2: x1 + x2, y, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br><span class="line"><span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<p>In the case above, the first step of work by <code>reduce</code> is to apply the given function with <code>init</code> 0  and the first element of the <code>iter</code> y as input(so the given function must receive 2 parameters) to get the result 1. Then apply the function with the result 1 and the second element 2, and repeat it until the last one of the list.</p>
<p>If no <code>init</code> was given, it would first apply the given function with the first two elements of the <code>iter</code> y.</p>
<h3 id="xlength"><a href="#xlength" class="headerlink" title="xlength()"></a>xlength()</h3><p>The source code of xlength:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xlength</span>(<span class="params">y</span>):</span></span><br><span class="line">	<span class="keyword">return</span> reduce(<span class="keyword">lambda</span> <span class="built_in">sum</span>, element: <span class="built_in">sum</span> + <span class="number">1</span>, y, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>It really confused me when I first ran this code and found that it returns the length of the list. Then I realized it that it works by the interesting applying of <code>init</code> and <code>func</code>. The <code>func</code> desert the <code>element</code>, which means it has no effect on the results the <code>func</code> returns. So the <code>sum</code> is in some ways a counter to count the length of the <code>iter</code> by <code>sum + 1</code> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/10/xlength-interesting-function/" data-id="clsk6sw3g0011cf262a284j8g" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/functools/" rel="tag">functools</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lambdaFunction/" rel="tag">lambdaFunction</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-sharement-of-paremeters-between-shell-scripts" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/02/07/sharement-of-paremeters-between-shell-scripts/" class="article-date">
  <time datetime="2022-02-07T05:51:05.000Z" itemprop="datePublished">2022-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/07/sharement-of-paremeters-between-shell-scripts/">sharement of paremeters between shell scripts</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>Today some problem occurs when I want to write a shell script that helps me <code>cd</code> to some deep directories which are long enough for me, a lazy man, to write a script. I found that <code>cd</code> does’t work in a <code>.sh</code> file when I use <code>bash</code> to execute it. Then I learned that a child shell is created to execute the commands in the scripts when using <code>bash</code>. To solve this problem, the most fundmental way is to use <code>source</code>, which won’t create a child shell.</p>
<p>After solving it, I wondered that how does two shell scripts share their parameters when one script will create a child shell in another’s child shell, which means that you can’t just use <code>bash config.sh</code> to tell the parent shell what the paraments in the <code>config.sh</code> are, and using <code>source config.sh</code> won’t tell the parent shell’s another child shell(e.g. step1.sh) what the paraments are.(like stucture below) </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#in the `parent.sh`</span></span><br><span class="line"><span class="built_in">source</span> config.sh</span><br><span class="line"></span><br><span class="line">bash step1.sh</span><br><span class="line"><span class="comment">#`step1.sh` won&#x27;t get the parameters in the `copnfig.sh`</span></span><br></pre></td></tr></table></figure>
<h1 id="Solvement"><a href="#Solvement" class="headerlink" title="Solvement"></a>Solvement</h1><p>There are three kinds of variable in shell scripts:</p>
<ul>
<li>local variable: difined by <code>local</code> statement, scope: the function in which it is defined</li>
<li>global variable: default, scope: the shell(not including child shells) </li>
<li>environment variable: defined by <code>export</code> command, scope: the shell(including child shells)</li>
</ul>
<p>( from<a target="_blank" rel="noopener" href="https://ningyuwhut.github.io/cn/2019/06/share-shell-variable-between-scripts">ningyuwhut’s blog</a>)</p>
<p>Figuring out these three kinds of variable helps a lot in solving the probelms above.</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>I used three shell scripts to show it.</p>
<ol>
<li>child_globalconfig.sh<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#It is child_globalconfig.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#-*-coding:UTF-8-*-</span></span><br><span class="line"><span class="comment">#Author: LeafLight</span></span><br><span class="line"><span class="comment">#Date: 2022-02-07</span></span><br><span class="line">srcp=<span class="string">&quot;successful&quot;</span></span><br><span class="line"><span class="built_in">export</span> envp=<span class="string">&quot;SUCCESSFUL&quot;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>2.childShell.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#It is childShell.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#-*-coding:UTF-8-*-</span></span><br><span class="line"><span class="comment">#Author: LeafLight</span></span><br><span class="line"><span class="comment">#Date: 2022-02-07</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;here is the child shell to show the advantage of using export to create environment parameter.&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;envp: <span class="variable">$envp</span>&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;srcp: <span class="variable">$srcp</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>3.parentShell.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#It is parentShell.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#-*-coding:UTF-8-*-</span></span><br><span class="line"><span class="comment">#Author: LeafLight</span></span><br><span class="line"><span class="comment">#Date: 2022-02-07</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;---&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;here is the parent shell&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;load the config by source&quot;</span></span><br><span class="line"><span class="built_in">source</span> ./child_globalconfig.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;---&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;parent shell tries to echo the srcp and envp&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;envp: <span class="variable">$envp</span>&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;srcp: <span class="variable">$srcp</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;---&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;child shell(by bash)&quot;</span></span><br><span class="line">bash ./childShell.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;---&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;child shell(by source)&quot;</span></span><br><span class="line"><span class="built_in">source</span> ./childShell.sh</span><br></pre></td></tr></table></figure>
<p>4.Running the <code>parentShell.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ bash parentShell.sh</span><br><span class="line">---</span><br><span class="line">here is the parent shell</span><br><span class="line">load the config by <span class="built_in">source</span></span><br><span class="line">---</span><br><span class="line">parent shell tries to <span class="built_in">echo</span> the srcp and envp</span><br><span class="line">envp: SUCCESSFUL</span><br><span class="line">srcp: successful</span><br><span class="line">---</span><br><span class="line">child shell(by bash)</span><br><span class="line">here is the child shell to show the advantage of using <span class="built_in">export</span> to create environment parameter.</span><br><span class="line">envp: SUCCESSFUL</span><br><span class="line">srcp: </span><br><span class="line">---</span><br><span class="line">child shell(by <span class="built_in">source</span>)</span><br><span class="line">here is the child shell to show the advantage of using <span class="built_in">export</span> to create environment parameter.</span><br><span class="line">envp: SUCCESSFUL</span><br><span class="line">srcp: successful</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/02/07/sharement-of-paremeters-between-shell-scripts/" data-id="clsk6sw3f000wcf265azahmi7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/study-note/" rel="tag">study note</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-GAN-learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/14/GAN-learning/" class="article-date">
  <time datetime="2022-01-14T06:47:22.000Z" itemprop="datePublished">2022-01-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MachineLearning/">MachineLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/14/GAN-learning/">GAN_learning(PLAN)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="What-is-GAN"><a href="#What-is-GAN" class="headerlink" title="What is GAN?"></a>What is GAN?</h1><p><em>GAN</em> is the abbreviation of <em>Generative Adversarial Network</em>.It is a model of <strong>structure learning</strong></p>
<h1 id="Why-do-I-learn-GAN"><a href="#Why-do-I-learn-GAN" class="headerlink" title="Why do I learn GAN?"></a>Why do I learn GAN?</h1><p>GAN caught my attention when I read a paper about designing drugs causing desired transciptome change by <em>Bidirectional Adversarial Autoencoder(BiAA)</em>.Then I became interesting in the base the BiAA built upon,which is <em>GAN</em> and <em>Autoencoder</em>.</p>
<p><em>cited from doi:10.3389/fphar.2020.00269</em></p>
<blockquote>
<p>Gene expression profiles are useful for assessing the efficacy and side effects of drugs. In this paper, we propose a new generative model that infers drug molecules that could induce a desired change in gene expression.</p>
</blockquote>
<p>Here are the reason why I want to study further and deeper about this model</p>
<ul>
<li>The model’s structure is interesting which contains a generator and a discriminator that help with each other. </li>
<li>It is becoming more and more popular at a crazy speed.(Though many variations of it exist,it seems that no huge breakthough is made.)</li>
<li>It may help with the exploration of a brand new way of drug design.</li>
<li>The teacher Hongyi Li is so interesting.</li>
</ul>
<h1 id="Things-I-want-to-learn-about-GAN"><a href="#Things-I-want-to-learn-about-GAN" class="headerlink" title="Things I want to learn about GAN"></a>Things I want to learn about GAN</h1><ul>
<li>how it works by mathematic</li>
<li>the already-realized and promising application of it</li>
<li>how to use it in the field of medicine and how to make it more useful and dependable</li>
</ul>
<h1 id="Things-I-need-to-do"><a href="#Things-I-need-to-do" class="headerlink" title="Things I need to do"></a>Things I need to do</h1><ul>
<li>Learning the online courses by Hongyi Li</li>
<li>Read and try to understand the source code of <a target="_blank" rel="noopener" href="https://github.com/insilicomedicine/BiAAE.git">BiAA</a></li>
<li>Add BiAA to my software or come up with a better model</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/14/GAN-learning/" data-id="clsk6sw3b0007cf2615c6eimv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PLAN/" rel="tag">PLAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/structure-learning/" rel="tag">structure learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-hexo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/14/hello-hexo/" class="article-date">
  <time datetime="2022-01-14T05:30:52.000Z" itemprop="datePublished">2022-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/14/hello-hexo/">hello hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Header-1-test"><a href="#Header-1-test" class="headerlink" title="Header 1 test"></a>Header 1 test</h1><h2 id="Header-2-test"><a href="#Header-2-test" class="headerlink" title="Header 2 test"></a>Header 2 test</h2><h3 id="Header-3-test"><a href="#Header-3-test" class="headerlink" title="Header 3 test"></a>Header 3 test</h3><p><em>italic test</em></p>
<p><em>bold test</em>_</p>
<p><a href="google.com">url test(google.com-failed)</a><br><a href="www.google.com">url test(www.google.com-failed)</a><br><a target="_blank" rel="noopener" href="https://www.google.com/">url test(https://www.google.com)</a></p>
<ul>
<li>unordered</li>
<li>list</li>
<li>test</li>
</ul>
<ol>
<li>ordered</li>
<li>list </li>
<li>test</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/14/hello-hexo/" data-id="clsk6sw3f000tcf268jqbd8sq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hello-world/" rel="tag">hello world</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-test" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/14/test/" class="article-date">
  <time datetime="2022-01-14T05:25:12.000Z" itemprop="datePublished">2022-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/14/test/">test</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/14/test/" data-id="clsk6sw3f000ycf265l3m8jsv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Math/MathModel/">MathModel</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python/GeneralNote/">GeneralNote</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/APH/" rel="tag">APH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CMap/" rel="tag">CMap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Copy/" rel="tag">Copy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset/" rel="tag">Dataset</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/" rel="tag">Entropy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GrammarVAE/" rel="tag">GrammarVAE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Himmelblau-Function/" rel="tag">Himmelblau Function</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-theory/" rel="tag">Information theory</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MNIST/" rel="tag">MNIST</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MathModel/" rel="tag">MathModel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Note/" rel="tag">Note</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PLAN/" rel="tag">PLAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pokemon/" rel="tag">Pokemon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Practice/" rel="tag">Practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Source-Code/" rel="tag">Source Code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Utilities/" rel="tag">Utilities</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VAE/" rel="tag">VAE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WorkFlow/" rel="tag">WorkFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Workflow/" rel="tag">Workflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/functools/" rel="tag">functools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hello-world/" rel="tag">hello world</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lambdaFunction/" rel="tag">lambdaFunction</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/molecules/" rel="tag">molecules</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nltk/" rel="tag">nltk</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/note/" rel="tag">note</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/structure-learning/" rel="tag">structure learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/study-note/" rel="tag">study note</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/APH/" style="font-size: 10px;">APH</a> <a href="/tags/CMap/" style="font-size: 10px;">CMap</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Copy/" style="font-size: 10px;">Copy</a> <a href="/tags/Dataset/" style="font-size: 10px;">Dataset</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/GrammarVAE/" style="font-size: 10px;">GrammarVAE</a> <a href="/tags/Himmelblau-Function/" style="font-size: 10px;">Himmelblau Function</a> <a href="/tags/Information-theory/" style="font-size: 10px;">Information theory</a> <a href="/tags/MNIST/" style="font-size: 10px;">MNIST</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/MachineLearning/" style="font-size: 20px;">MachineLearning</a> <a href="/tags/Math/" style="font-size: 10px;">Math</a> <a href="/tags/MathModel/" style="font-size: 10px;">MathModel</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Note/" style="font-size: 15px;">Note</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/PLAN/" style="font-size: 15px;">PLAN</a> <a href="/tags/Pokemon/" style="font-size: 10px;">Pokemon</a> <a href="/tags/Practice/" style="font-size: 15px;">Practice</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Pytorch/" style="font-size: 20px;">Pytorch</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/Source-Code/" style="font-size: 10px;">Source Code</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Utilities/" style="font-size: 10px;">Utilities</a> <a href="/tags/VAE/" style="font-size: 10px;">VAE</a> <a href="/tags/WorkFlow/" style="font-size: 10px;">WorkFlow</a> <a href="/tags/Workflow/" style="font-size: 15px;">Workflow</a> <a href="/tags/functools/" style="font-size: 10px;">functools</a> <a href="/tags/hello-world/" style="font-size: 10px;">hello world</a> <a href="/tags/lambdaFunction/" style="font-size: 10px;">lambdaFunction</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/molecules/" style="font-size: 10px;">molecules</a> <a href="/tags/nltk/" style="font-size: 10px;">nltk</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/structure-learning/" style="font-size: 10px;">structure learning</a> <a href="/tags/study-note/" style="font-size: 10px;">study note</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/02/13/PythonNote/">Python Learning Note</a>
          </li>
        
          <li>
            <a href="/2024/02/13/Pokemon_dataset_load_WorkFlow/">Custom Dataset--Pokemon dataset loading by Pytorch</a>
          </li>
        
          <li>
            <a href="/2024/02/13/HimmelblauOptimization_Practice_WorkFlow/">Himmelblau Function -- Optimization Practice</a>
          </li>
        
          <li>
            <a href="/2024/02/13/CNN_practice_WorkFlow/">CNN Practice Workflow</a>
          </li>
        
          <li>
            <a href="/2024/02/13/EntropyAndCrossEntropy/">An interesting understanding of Entropy and Cross Entropy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 LeafLight<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>